{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup\n",
    "import tools we need and set up the variables of urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import math\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import re\n",
    "import random\n",
    "import geopandas\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables applied to whole project\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing \n",
    "In this part, we will firstly write a function to calculate the distance from pickup address to drop off address by using the lattitude and longtitude. Secondly, we read each data from website or csv. Thirdly, we clean each dataframe and add distance column in it. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculating distance\n",
    "Here we do 2 things to help us to do the following work:\n",
    "1. write a function to calculate distance of 2 given point with lattitude and longtitube.\n",
    "2. write a function which can add a column named distance with the above results into our future dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(from_coord, to_coord):\n",
    "     \n",
    "    ''' Take in 2 coordinates with lattitudes and longtitudes\n",
    "        Return a float variable named distance which is the distance of the 2 coordinated we took in '''\n",
    "    \n",
    "    #calculate the difference between each lattitude and longtitude\n",
    "    dlat = to_coord[0] - from_coord[0]\n",
    "    dlon = to_coord[1] - from_coord[1]\n",
    "\n",
    "    #use math formula to calculate the distance\n",
    "    a = (math.sin(dlat/2))**2 + math.cos(from_coord[0]) * math.cos(to_coord[0]) * (math.sin(dlon/2))**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    distance = 6371 * c\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(dataframe):\n",
    "\n",
    "    ''' Take in dataframe, Return a new dataframe with additional column named distance '''\n",
    "\n",
    "    #add a distance column to both uber and taxi files\n",
    "    dis = []\n",
    "    for i in range(len(dataframe.index)):\n",
    "        dis.append(calculate_distance((dataframe.iloc[i]['PU_lat'],dataframe.iloc[i]['PU_lon']), (dataframe.iloc[i]['DO_lat'],dataframe.iloc[i]['DO_lon'])))\n",
    "    \n",
    "    dataframe.insert(len(dataframe.columns), \"distance\", dis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Processing Taxi Data\n",
    "\n",
    "Here we download taxi data from website, clean the data by deleting the null raws and add a new column named distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_taxi_csv_urls():\n",
    "\n",
    "    '''Return a list including urls of parquet files'''\n",
    "    \n",
    "    response = requests.get(TAXI_URL)\n",
    "    soup = bs4.BeautifulSoup(response.content, 'html.parser')\n",
    "    yellow_a_tags = soup.find_all(\"a\", attrs={\"title\": \"Yellow Taxi Trip Records\"})\n",
    "    yellow_taxis = [a[\"href\"] for a in yellow_a_tags] # all yellow taxi trip records urls\n",
    "    \n",
    "    # creating a pattern to filter url for desired time frame from January 2009 to June 2015\n",
    "    pattern1 = re.compile(r\"yellow_tripdata_2009-([0-9]{2}).parquet\")\n",
    "    pattern2 = re.compile(r\"yellow_tripdata_201([0-4]{1})-([0-9]{2}).parquet\")\n",
    "    pattern3 = re.compile(r\"yellow_tripdata_2015-0([1-6]{1}).parquet\")\n",
    "    \n",
    "    filtered_taxi_files = []\n",
    "    \n",
    "    #finding all yellow taxi record urls from January 2009 to June 2015\n",
    "    for link in yellow_taxis:\n",
    "        match = pattern1.search(link)\n",
    "        if match:\n",
    "            filtered_taxi_files.append(match.string)\n",
    "    \n",
    "    for link in yellow_taxis:\n",
    "        match = pattern2.search(link)\n",
    "        if match:\n",
    "            filtered_taxi_files.append(match.string)\n",
    "    \n",
    "    for link in yellow_taxis:\n",
    "        match = pattern3.search(link)\n",
    "        if match:\n",
    "            filtered_taxi_files.append(match.string)\n",
    "    \n",
    "    return filtered_taxi_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd626900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the taxi_zones geo files and extract the lat/log of the center point of each polygon\n",
    "taxi_zones = \"taxi_zones.shp\"\n",
    "zones = geopandas.read_file(taxi_zones)\n",
    "zones = zones.to_crs(4326)\n",
    "zones[\"lon\"] = zones.centroid.x\n",
    "zones[\"lat\"] = zones.centroid.y\n",
    "latitudes = zones.set_index('LocationID').to_dict()['lat']\n",
    "longitudes = zones.set_index('LocationID').to_dict()['lon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month_taxi_data(url):\n",
    "   \n",
    "    '''Take in url, Return dataframe'''\n",
    "    \n",
    "    #download and read url into dataframe\n",
    "    r = requests.get(url, stream = [True])\n",
    "    file_name = ''.join(c for c in url if c.isdigit()) + \"_yellowtaxi\"\n",
    "    with open(file_name, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    df = pd.read_parquet(file_name)\n",
    "    \n",
    "    if \"PULocationID\" in df.columns:\n",
    "        df.insert(9, \"PU_lon\", df[\"PULocationID\"].map(longitudes))\n",
    "        df.insert(10, \"PU_lat\", df[\"PULocationID\"].map(latitudes))\n",
    "        df.insert(11, \"DO_lon\",df[\"DOLocationID\"].map(longitudes))\n",
    "        df.insert(12, \"DO_lat\", df[\"DOLocationID\"].map(latitudes))\n",
    "    \n",
    "    #delete the unwanted columns\n",
    "    if \"vendor_id\" in df.columns:\n",
    "        df.drop(['vendor_id', 'passenger_count', 'trip_distance', 'rate_code', 'store_and_fwd_flag', 'payment_type', 'fare_amount', 'surcharge', 'mta_tax','tolls_amount'], axis=1, inplace=True)\n",
    "        df = df.rename(columns = {\n",
    "            'pickup_datetime' : 'PU_datetime',\n",
    "            'dropoff_datetime' : 'DO_datetime',\n",
    "            'pickup_longitude' : 'PU_lon',\n",
    "            'pickup_latitude' : 'PU_lat',\n",
    "            'dropoff_longitude' : 'DO_lon',\n",
    "            'dropoff_latitude' : 'DO_lat',\n",
    "            'tip_amount' : 'tip_amt',\n",
    "            'total_amount' : 'total_amt', \n",
    "        })\n",
    "    if \"vendor_name\" in df.columns:\n",
    "        df.drop(['vendor_name', 'Passenger_Count', 'Trip_Distance', 'Rate_Code', 'store_and_forward', 'Payment_Type', 'Fare_Amt', 'surcharge', 'mta_tax', 'Tolls_Amt' ], axis=1, inplace=True)\n",
    "        df = df.rename(columns = {\n",
    "            'Trip_Pickup_DateTime' : 'PU_datetime',\n",
    "            'Trip_Dropoff_DateTime' : 'DO_datetime',\n",
    "            'Start_Lon' : 'PU_lon',\n",
    "            'Start_Lat' : 'PU_lat',\n",
    "            'End_Lon' : 'DO_lon',\n",
    "            'End_Lat' : 'DO_lat',\n",
    "            'Tip_Amt' : 'tip_amt',\n",
    "            'Total_Amt' : 'total_amt', \n",
    "        })\n",
    "    if \"VendorID\" in df.columns:\n",
    "        df.drop(['VendorID', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tolls_amount', 'improvement_surcharge', 'congestion_surcharge', 'airport_fee'], axis=1, inplace=True)\n",
    "        df = df.rename(columns = {\n",
    "            'tpep_pickup_datetime' : 'PU_datetime',\n",
    "            'tpep_dropoff_datetime' : 'DO_datetime',\n",
    "            'tip_amount' : 'tip_amt',\n",
    "            'total_amount' : 'total_amt', \n",
    "        })\n",
    "        \n",
    "    # delete any rows with null input\n",
    "    df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)\n",
    "    \n",
    "    # filter data to be within desire pick-up and drop-off locations\n",
    "    df = df[(df['PU_lat']>40.560445) & (df['PU_lat']<40.908524)]\n",
    "    df = df[(df['DO_lat']>40.560445) & (df['DO_lat']<40.908524)]\n",
    "    df = df[(df['PU_lon']> -74.242330) & (df['PU_lon']< -73.717047)]\n",
    "    df = df[(df['DO_lon']> -74.242330) & (df['DO_lon']< -73.717047)]\n",
    "    \n",
    "    \n",
    "    #sample 2500 rows from each file\n",
    "    df_sample = df.sample(2500)\n",
    "    \n",
    "    return df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data():\n",
    "\n",
    "    '''Return dataframe'''\n",
    "    \n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    all_csv_urls = find_taxi_csv_urls()\n",
    "    for csv_url in all_csv_urls:\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_month_taxi_data(csv_url)\n",
    "        add_distance_column(dataframe)\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        \n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    return taxi_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data\n",
    "\n",
    "In this part, we read uber data from cvs file. Then we clean data by deleting raws with null value and unnecessary columns, filter the data by condition, and add a distance column to new dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "\n",
    "    '''Take in cvs file, Return dataframe'''\n",
    "\n",
    "    #read uber data from cvs file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    #delete raws with null values\n",
    "    df = df[~(df == 0).any(axis=1)]\n",
    "\n",
    "    #filter data which lattitube in range(40.560445,40.908524) and longtitude in range(-74.242330ï¼Œ-73.717047)\n",
    "    df = df[(df['pickup_latitude']>40.560445) & (df['pickup_latitude']<40.908524)]\n",
    "    df = df[(df['dropoff_latitude']>40.560445) & (df['dropoff_latitude']<40.908524)]\n",
    "    df = df[(df['pickup_longitude']> -74.242330) & (df['pickup_longitude']< -73.717047)]\n",
    "    df = df[(df['dropoff_longitude']> -74.242330) & (df['dropoff_longitude']< -73.717047)]\n",
    "\n",
    "    #rename the columns\n",
    "    df = df.rename(columns = {\n",
    "            'pickup_datetime' : 'PU_datetime',\n",
    "            'pickup_longitude' : 'PU_lon',\n",
    "            'pickup_latitude' : 'PU_lat',\n",
    "            'dropoff_longitude' : 'DO_lon',\n",
    "            'dropoff_latitude' : 'DO_lat',\n",
    "        })\n",
    "\n",
    "    #change the data type to datetime\n",
    "    pd.to_datetime(df['PU_datetime'])\n",
    "    #delete unnecessary columns\n",
    "    del df['key']\n",
    "    del df['Unnamed: 0']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "\n",
    "    '''Return dataframe'''\n",
    "    \n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_CSV)\n",
    "    add_distance_column(uber_dataframe)\n",
    "    return uber_dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data\n",
    "\n",
    "Here we downloads the weather data and clean it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "      \n",
    "    '''Take in csv file, Return a dataframe'''\n",
    "    \n",
    "    #downloads csv file \n",
    "    df = pd.read_csv(csv_file, low_memory=False)\n",
    "\n",
    "    #change the data type to datetime\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "\n",
    "    #select the specific columns we need \n",
    "    df = df[['DATE', 'HourlyDewPointTemperature', 'HourlyPrecipitation', 'HourlyWindSpeed']]\n",
    "\n",
    "    #populate the data \n",
    "    df.replace(\"T\", 0.00, inplace=True)\n",
    "    df.replace(0.00, np.nan, inplace=True)\n",
    "\n",
    "    #delete the raws which are meaningless\n",
    "    df.dropna(how =\"any\", inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "     \n",
    "    '''Take in csv file, Return a dataframe'''\n",
    "    \n",
    "    #downloads csv file \n",
    "    df = pd.read_csv(csv_file, low_memory=False)\n",
    "\n",
    "    #change the data type to datetime\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "\n",
    "    #select the specific columns we need \n",
    "    df = df[['DATE','HourlyPrecipitation', 'HourlyWindSpeed']]\n",
    "\n",
    "    #clean the data\n",
    "    df.replace(\"T\", 0, inplace=True)\n",
    "    df['HourlyPrecipitation'] = df['HourlyPrecipitation'].str.rstrip('s')\n",
    "    df['HourlyPrecipitation'] = pd.to_numeric(df['HourlyPrecipitation'])\n",
    "    df['HourlyWindSpeed'] = pd.to_numeric(df['HourlyWindSpeed'])\n",
    "    df1 = df.groupby([df['DATE'].dt.date])['HourlyPrecipitation'].sum()\n",
    "    df2 = df.groupby([df['DATE'].dt.date])['HourlyWindSpeed'].mean()\n",
    "    \n",
    "    df3 = pd.read_csv(csv_file, low_memory=False)\n",
    "    df3['DATE'] = pd.to_datetime(df3['DATE'])\n",
    "    df3 = df3[['DATE','DailyAverageDewPointTemperature', 'DailySustainedWindSpeed']]\n",
    "    df3 = df3.groupby([df3['DATE'].dt.date]).sum()\n",
    "    \n",
    "    \n",
    "    frame = [df1, df2, df3]\n",
    "    df4 = pd.concat(frame, axis = 1)\n",
    "    df4 = df4.rename(columns = {'HourlyPrecipitation' : 'DailyPrecipitation',\n",
    "                               'HourlyWindSpeed' : 'DailyWindSpeed'\n",
    "                               })\n",
    "    df4.replace(np.nan, 0.00, inplace=True)\n",
    "    \n",
    "    return df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "     \n",
    "    '''Return two dataframes'''\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "    \n",
    "    weather_csv_files = [\"2009_weather.csv\", \"2010_weather.csv\", \"2011_weather.csv\", \"2012_weather.csv\", \"2013_weather.csv\", \n",
    "                        \"2014_weather.csv\", \"2015_weather.csv\"]\n",
    "    \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f900f7aa",
   "metadata": {},
   "source": [
    "### Process All Data\n",
    "\n",
    "Here we actually execute all the required functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data = get_and_clean_taxi_data()\n",
    "uber_data = get_uber_data()\n",
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data\n",
    "\n",
    "In part2, we create a database based on the data we processed in part1. We use sqlite to achive it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather(\n",
    "   hourly_weatherId INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "   DATE DATE,\n",
    "   HourlyDewPointTemperature FLOAT,\n",
    "   HourlyPrecipitation FLOAT\n",
    "   HourlyWindSpeed FLOAT\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather(\n",
    "   daily_weatherId INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "   DATE DATE,\n",
    "   DailyPrecipitation FLOAT,\n",
    "   DailyWindSpeed FLOAT\n",
    "   DailyAverageDewPointTemperature FLOAT\n",
    "   DailySustainedWindSpeed FLOAT\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips(\n",
    "   taxi_tripsId INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "   PU_datetime DATE,\n",
    "   DO_datetime DATE,\n",
    "   PU_lon FLOAT\n",
    "   PU_lat FLOAT\n",
    "   DO_lon FLOAT\n",
    "   DO_lat FLOAT\n",
    "   tip_amt FLOAT\n",
    "   total_amt FLOAT\n",
    "   distance FLOAT\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips(\n",
    "   uber_tripsId INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "   fare_amount FLOAT\n",
    "   PU_datetime DATE,\n",
    "   PU_lon FLOAT\n",
    "   PU_lat FLOAT\n",
    "   DO_lon FLOAT\n",
    "   DO_lat FLOAT\n",
    "   passenger_count INTEGER\n",
    "   distance FLOAT\n",
    ")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database\n",
    "\n",
    "We create database above and we want to add our data to the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    '''Take in dictionary'''\n",
    "    for table_name, df in table_to_df_dict.items():\n",
    "        df.to_sql(table_name, engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data\n",
    "We analyze the data we obtained above by answering these 6 questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_query_to_file(query, outfile):\n",
    "    with open(outfile, \"w\") as f:\n",
    "        f.write(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1\n",
    "\n",
    "_**TODO:** For 01-2009 through 06-2015, what hour of the day was the most popular to take a yellow taxi? The result should have 24 bins._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## most popular hour of the day to take a yellow taxi\n",
    "QUERY_1 = \"\"\"\n",
    "SELECT strftime('%H', PU_datetime) AS hour, COUNT(*) AS num\n",
    "FROM taxi_trips\n",
    "GROUP BY hour\n",
    "\n",
    "ORDER BY num DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, \"most_popular_taxi_hour.sql\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdf7be49",
   "metadata": {},
   "source": [
    "### Query 2\n",
    "\n",
    "_**TODO:** For the same time frame, what day of the week was the most popular to take an uber?_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1de8c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2 = \"\"\"\n",
    "SELECT strftime('%w', PU_datetime) As day, COUNT(*) AS num\n",
    "FROM uber_trips\n",
    "GROUP BY day\n",
    "\n",
    "ORDER BY num DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db58e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_2).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07705ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_2, \"most_popular_uber_day.sql\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d3cf7ed",
   "metadata": {},
   "source": [
    "### Query 3\n",
    "\n",
    "_**TODO:** What is the 95% percentile of distance traveled for all hired trips during July 2013?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aa45fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the 95% percentile of distance traveled for all hired trips during July 2013\n",
    "QUERY_3 = \"\"\"\n",
    "WITH all_hired_trips AS(\n",
    "SELECT PU_datetime, distance FROM taxi_trips \n",
    "WHERE PU_datetime BETWEEN \"2013-07-01\" AND \"2013-07-31\"\n",
    "UNION ALL\n",
    "SELECT PU_datetime, distance FROM uber_trips WHERE PU_datetime BETWEEN \"2013-07-01\" AND \"2013-07-31\")\n",
    "\n",
    "SELECT distance FROM all_hired_trips\n",
    "ORDER BY distance ASC\n",
    "\n",
    "LIMIT 1\n",
    "OFFSET (SELECT COUNT(*) FROM all_hired_trips) * 95 / 100 ;\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77681d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_3).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_3, \"95th Percentile of distance Jul13.sql\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edd5ecbb",
   "metadata": {},
   "source": [
    "### Query 4\n",
    "\n",
    "_**TODO:** What were the top 10 days with the highest number of hired rides for 2009, and what was the average distance for each day?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ea41f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the top 10 days with the highest number of hired rides for 2009, and the average distance for each day?\n",
    "QUERY_4 = \"\"\"WITH all_hired_trips AS (\n",
    "SELECT PU_datetime, distance FROM taxi_trips \n",
    "WHERE PU_datetime BETWEEN '2009-01-01' AND '2009-12-31'\n",
    "UNION ALL\n",
    "SELECT PU_datetime,distance FROM uber_trips WHERE PU_datetime BETWEEN '2009-01-01' AND '2009-12-31')\n",
    "\n",
    "SELECT PU_datetime AS date, AVG(distance) AS avg_dist, COUNT(*) AS num\n",
    "FROM all_hired_trips\n",
    "\n",
    "GROUP BY date\n",
    "ORDER BY num DESC\n",
    "\n",
    "LIMIT 10;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e923063",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_4).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d659b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_4, \"top 10 most rides 2009.sql\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "060cd22a",
   "metadata": {},
   "source": [
    "### Query 5\n",
    "\n",
    "_**TODO:** Which 10 days in 2014 were the windiest, and how many hired trips were made on those days?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41d3d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#top 10 windest days in 2014, and number of hired trips on those days\n",
    "QUERY_5 =\"\"\"\n",
    "SELECT date(PU_datetime) AS date, COUNT(*) AS num\n",
    "\n",
    "FROM(SELECT PU_datetime FROM taxi_trips\n",
    "UNION ALL\n",
    "SELECT PU_datetime FROM uber_trips)\n",
    "\n",
    "GROUP BY date\n",
    "HAVING date IN (SELECT DATE FROM daily_weather WHERE DATE BETWEEN '2014-01-01' AND '2014-12-31'\n",
    "ORDER BY DailyAverageWindSpeed DESC)\n",
    "\n",
    "LIMIT 10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e88044",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_5).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6450b818",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_5, \"top 10 windest day 2014.sql\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a04f38de",
   "metadata": {},
   "source": [
    "### Query 6\n",
    "\n",
    "_**TODO:** During Hurricane Sandy in NYC (Oct 29-30, 2012) and the week leading up to it, how many trips were taken each hour, and for each hour, how much precipitation did NYC receive and what was the sustained wind speed?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7578d3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#During Hurricane Sandy in NYC (Oct 29-30, 2012) and the week leading up to it, \n",
    "#trips taken each hour, and for each hour, the amount of precipitation in NYC and the sustained wind speed\n",
    "QUERY_6 = \"\"\"\n",
    "WITH all_hired_trips AS (\n",
    "SELECT strftime('%Y-%m-%d %H',PU_datetime) AS date\n",
    "FROM taxi_trips\n",
    "WHERE PU_datetime BETWEEN '2012-10-22' AND '2012-11-06'\n",
    "UNION ALL\n",
    "SELECT strftime('%Y-%m-%d %H',PU_datetime) FROM uber_trips AS date\n",
    "WHERE PU_datetime BETWEEN '2012-10-22' AND '2012-11-07')\n",
    "\n",
    "SELECT strftime('%Y-%m-%d %H', DATE) AS day, COALESCE(COUNT(day),0) AS num, HourlyPrecipitation, HourlyWindSpeed FROM hourly_weather\n",
    "\n",
    "LEFT JOIN all_hired_trips ON day\n",
    "WHERE day BETWEEN '2012-10-22' AND '2012-11-06'\n",
    "\n",
    "GROUP BY day\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bfa06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_6).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69c4844",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_6, \"Hurricane days weather and rides.sql\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data\n",
    "\n",
    "Here we plot 6 graphs to help readers analyze the above data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1\n",
    "\n",
    "_Visualization demonstrating the numbers of trips in each hour of the day through 1/1/2009 - 6/1/2015_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c448aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the query from before into dataframe\n",
    "df = pd.read_sql_query(QUERY_1, engine)\n",
    "\n",
    "#producing a bar chart\n",
    "df.plot.bar(x='hour', y='num', title = \"Number of ride in each hour of the day through 1/1/2009 - 6/1/2015\", xlabel = \"Hour of the day\", ylabel = \"number of rides\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e85683d",
   "metadata": {},
   "source": [
    "### Visualization 2\n",
    "\n",
    "_Visualization demonstrating the average distance traveled per month and 95% confidence interval_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0d3798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use query to select and merge average distance by datetime column from two tables\n",
    "QUERY_avg_dis = \"\"\"\n",
    "WITH all_hired_trips AS (\n",
    "SELECT strftime('%m', PU_datetime) AS month, AVG(distance) AS dis FROM taxi_trips\n",
    "GROUP BY month\n",
    "UNION ALL\n",
    "SELECT strftime('%m', PU_datetime) AS month, AVG(distance) AS dis FROM uber_trips)\n",
    "\n",
    "SELECT dis FROM all_hired_trips\n",
    "\n",
    "ORDER BY dis ASC\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#read the query into dataframe\n",
    "df = pd.read_sql_query(QUERY_avg_dis, engine)\n",
    "\n",
    "#find the 95% interval range (one side)\n",
    "ci = 1.96 * np.std(df['dis'])/np.sqrt(len(df.index))\n",
    "\n",
    "#producing a graph with line and shaded area indicating the 95% interval\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(df.index,df['dis'])\n",
    "ax.fill_between(df.index, (df['dis']-ci), (df['dis']+ci), alpha=.1)\n",
    "ax.set_title(\"Average distance of rides per month and it 95% confidence Interval\")\n",
    "ax.set_xlabel(\"Month of the Year\")\n",
    "ax.set_ylabel(\"Average Distance of Rides\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2386711",
   "metadata": {},
   "source": [
    "### Visualization 3\n",
    "\n",
    "_Visualization to compare what day of the week was most popular for drop offs for each airport: JFK, LGA, EWR_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bdd024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f26761ee",
   "metadata": {},
   "source": [
    "### Visualization 4\n",
    "\n",
    "_Create a heatmap of all hired trips over a map of the area_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7c553a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60745f8e",
   "metadata": {},
   "source": [
    "### Visualization 5\n",
    "\n",
    "_Visualization comparesing tip amount versus distance._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77abf410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use query to select tips and distance columns from taxi_trips table\n",
    "QUERY_tip_dis = \"\"\"\n",
    "SELECT tip_amt AS tips, distance AS dis FROM taxi_trips\n",
    "\"\"\"\n",
    "#read the query into dataframe\n",
    "df = pd.read_sql_query(QUERY_tip_dis, engine)\n",
    "#obtain a small sample for a better visual\n",
    "df.sample(1000)\n",
    "#producing a scatter plot\n",
    "df.plot.scatter(x='tips', y='dis', title = \"tip amount vs trip distance for all taxi trips\", xlabel = \"Tip Amount\", ylabel = \"Trip Distance\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8074272b",
   "metadata": {},
   "source": [
    "### Visualization 6\n",
    "\n",
    "_Visualization comparing tip amount versus precipitation amount._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a08ed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use query to select the columns we need from each table \n",
    "QUERY_tips =\"\"\"SELECT strftime('%H', PU_datetime) AS hour, tip_amt FROM taxi_trips\"\"\" \n",
    "QUERY_prec =\"\"\"SELECT strftime('%H', DATE) AS hour, HourlyPrecipitation FROM hourly_weather\"\"\"\n",
    "\n",
    "#read the query into two dataframes\n",
    "tips_data = pd.read_sql_query(QUERY_tips, engine)\n",
    "precipitation_data = pd.read_sql_query(QUERY_prec, engine)\n",
    "\n",
    "#merge the two by the same hour records\n",
    "df = pd.merge(tips_data, precipitation_data, on = 'hour')\n",
    "#obtain a small sample for a better visual\n",
    "df = df.sample(1000)\n",
    "#drop rows with zero tip or zero precipitation\n",
    "df = df[(df[['tip_amt','HourlyPrecipitation']] != 0).all(axis=1)]\n",
    "#producing a scatter plot\n",
    "df.plot.scatter(x=\"HourlyPrecipitation\", y=\"tip_amt\", title=\"Hourly Precipitation vs Tip Amount\", xlabel = \"Hourly Precipitation\", ylabel = \"Tip Amount\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
